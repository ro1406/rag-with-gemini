{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) with Gemini 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue_G9ZU80ON0"
      },
      "source": [
        "- `google-genai`:  Google Gen AI python library\n",
        "- `PyPDF2`: To read PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install --upgrade --quiet google-genai PyPDF2 gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "# For asynchronous operations\n",
        "import asyncio\n",
        "\n",
        "# For data processing\n",
        "import glob\n",
        "from typing import Any\n",
        "\n",
        "from IPython.display import Audio, Markdown, display\n",
        "import PyPDF2\n",
        "\n",
        "# For GenerativeAI\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.genai.types import LiveConnectConfig\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For similarity score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# For retry mechanism\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV5bFDTVE3oX"
      },
      "source": [
        "#### Initialize Gen AI client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pjBP_V7JqhD"
      },
      "source": [
        "- Client for calling the Gemini API in Vertex AI\n",
        "- `vertexai=True`, indicates the client should communicate with the Vertex AI API endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEhq_4GBEW2a"
      },
      "outputs": [],
      "source": [
        "# Vertex AI API\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf93d5f0ce00"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-exp\"  # @param {type:\"string\", isTemplate: true}\n",
        "MODEL = (\n",
        "    f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{MODEL_ID}\"\n",
        ")\n",
        "\n",
        "text_embedding_model = \"text-embedding-005\"  # @param {type:\"string\", isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZZLuCecsp0e"
      },
      "source": [
        "#### Context Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWrK7HHjssqB"
      },
      "source": [
        "- Download the documents from Google Cloud Storage bucket\n",
        "- These documents are specific to `Cymbal Bikes` store\n",
        "  - [`Cymbal Bikes Return Policy`](https://storage.googleapis.com/github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesReturnPolicy.pdf): Contains information about return policy\n",
        "  - [`Cymbal Bikes Services`](https://storage.googleapis.com/github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesServices.pdf): Contains information about services provided by Cymbal Bikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLhNfYfYspnC"
      },
      "outputs": [],
      "source": [
        "!gsutil cp \"gs://github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesReturnPolicy.pdf\" \"documents/CymbalBikesReturnPolicy.pdf\"\n",
        "!gsutil cp \"gs://github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesServices.pdf\" \"documents/CymbalBikesServices.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOFNGNGjjEzD"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLqbaZjoCzng"
      },
      "outputs": [],
      "source": [
        "query = \"What is the price of a basic tune-up at Cymbal Bikes?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=query,\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB4U6s1-UlFw"
      },
      "source": [
        "### Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvUJzbgPM26m"
      },
      "source": [
        "For text generation, you need to set the `response_modalities` to `TEXT`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQOurRs5UU9p"
      },
      "outputs": [],
      "source": [
        "async def generate_content(query: str) -> str:\n",
        "    \"\"\"Function to generate text content using Gemini live API.\n",
        "\n",
        "    Args:\n",
        "      query: The query to generate content for.\n",
        "\n",
        "    Returns:\n",
        "      The generated content.\n",
        "    \"\"\"\n",
        "    config = LiveConnectConfig(response_modalities=[\"TEXT\"])\n",
        "\n",
        "    async with client.aio.live.connect(model=MODEL, config=config) as session:\n",
        "\n",
        "        await session.send(input=query, end_of_turn=True)\n",
        "\n",
        "        response = []\n",
        "        async for message in session.receive():\n",
        "            try:\n",
        "                if message.text:\n",
        "                    response.append(message.text)\n",
        "            except AttributeError:\n",
        "                pass\n",
        "\n",
        "            if message.server_content.turn_complete:\n",
        "                response = \"\".join(str(x) for x in response)\n",
        "                return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye1TwWVaVSxF"
      },
      "source": [
        "- Try a specific query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGqsp6nFDNsG"
      },
      "outputs": [],
      "source": [
        "query = \"What is the price of a basic tune-up at Cymbal Bikes?\"\n",
        "\n",
        "response = await generate_content(query)\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX9k92TlJ864"
      },
      "source": [
        "## Enhancing LLM Accuracy with RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5CXTtsPEyJ0"
      },
      "source": [
        "### Context Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M22BSDb2Xxpb"
      },
      "outputs": [],
      "source": [
        "documents = glob.glob(\"documents/*\")\n",
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-0zlJ3_FRfa"
      },
      "source": [
        "#### Document Embedding and Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vun69x23FWiw"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(multiplier=1, max=120), stop=stop_after_attempt(4))\n",
        "def get_embeddings(\n",
        "    embedding_client: Any, embedding_model: str, text: str, output_dim: int = 768\n",
        ") -> list[float]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for text with retry logic for API quota management.\n",
        "\n",
        "    Args:\n",
        "        embedding_client: The client object used to generate embeddings.\n",
        "        embedding_model: The name of the embedding model to use.\n",
        "        text: The text for which to generate embeddings.\n",
        "        output_dim: The desired dimensionality of the output embeddings (default is 768).\n",
        "\n",
        "    Returns:\n",
        "        A list of floats representing the generated embeddings. Returns None if a \"RESOURCE_EXHAUSTED\" error occurs.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Any exception encountered during embedding generation, excluding \"RESOURCE_EXHAUSTED\" errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        #!TBD IN WORKSHOP\n",
        "        pass\n",
        "    \n",
        "    except Exception as e:\n",
        "        if \"RESOURCE_EXHAUSTED\" in str(e):\n",
        "            return None\n",
        "        print(f\"Error generating embeddings: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2csDY5NsswwJ"
      },
      "source": [
        "- The code block executes the following steps:\n",
        "\n",
        "  - Extracts text from PDF documents and segments it into smaller chunks for processing.\n",
        "  - Employs a Vertex AI model to transform each text chunk into a numerical embedding vector, facilitating semantic representation and search.\n",
        "  - Constructs a Pandas DataFrame to store the embeddings, enriched with metadata such as document name and page number, effectively creating a searchable index for efficient retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TJlvdIsRfmX"
      },
      "outputs": [],
      "source": [
        "def build_index(\n",
        "    document_paths: list[str],\n",
        "    embedding_client: Any,\n",
        "    embedding_model: str,\n",
        "    chunk_size: int = 512,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build searchable index from a list of PDF documents with page-wise processing.\n",
        "\n",
        "    Args:\n",
        "        document_paths: A list of file paths to PDF documents.\n",
        "        embedding_client: The client object used to generate embeddings.\n",
        "        embedding_model: The name of the embedding model to use.\n",
        "        chunk_size: The maximum size (in characters) of each text chunk.  Defaults to 512.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame where each row represents a text chunk.  The DataFrame includes columns for:\n",
        "            - 'document_name': The path to the source PDF document.\n",
        "            - 'page_number': The page number within the document.\n",
        "            - 'page_text': The full text of the page.\n",
        "            - 'chunk_number': The chunk number within the page.\n",
        "            - 'chunk_text': The text content of the chunk.\n",
        "            - 'embeddings': The embedding vector for the chunk.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If no chunks are created from the input documents.\n",
        "        Exception:  Any exceptions encountered during file processing are printed to the console and the function continues to the next document.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    for doc_path in document_paths:\n",
        "        try:\n",
        "            with open(doc_path, \"rb\") as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                for page_num in range(len(pdf_reader.pages)):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    page_text = page.extract_text()\n",
        "\n",
        "                    #!TBD IN WORKSHOP\n",
        "                    pass\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document {doc_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not all_chunks:\n",
        "        raise ValueError(\"No chunks were created from the documents\")\n",
        "\n",
        "    return pd.DataFrame(all_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFGsl-Zvlej6"
      },
      "source": [
        "Let's create embeddings and an index using the provided documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjl5FDQckDcO"
      },
      "outputs": [],
      "source": [
        "vector_db_mini_vertex = build_index(\n",
        "    documents, embedding_client=client, embedding_model=text_embedding_model\n",
        ")\n",
        "vector_db_mini_vertex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZLX5ozMlxTX"
      },
      "outputs": [],
      "source": [
        "# Index size\n",
        "vector_db_mini_vertex.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvNVn3kT9FiB"
      },
      "outputs": [],
      "source": [
        "# Example of how a chunk looks like\n",
        "vector_db_mini_vertex.loc[0, \"chunk_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43txjyVlHT6v"
      },
      "source": [
        "#### Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88ndL_2wJ5ZD"
      },
      "outputs": [],
      "source": [
        "def get_relevant_chunks(\n",
        "    query: str,\n",
        "    vector_db: pd.DataFrame,\n",
        "    embedding_client: Any,\n",
        "    embedding_model: str,\n",
        "    top_k: int = 3,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant document chunks for a query using similarity search.\n",
        "\n",
        "    Args:\n",
        "        query: The search query string.\n",
        "        vector_db: A pandas DataFrame containing the vectorized document chunks.\n",
        "                     It must contain columns named 'embeddings', 'document_name',\n",
        "                     'page_number', and 'chunk_text'.\n",
        "                     The 'embeddings' column should contain lists or numpy arrays\n",
        "                     representing the embeddings.\n",
        "        embedding_client: The client object used to generate embeddings.\n",
        "        embedding_model: The name of the embedding model to use.\n",
        "        top_k: The number of most similar chunks to retrieve. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        A formatted string containing the top_k most relevant chunks.  Each chunk is\n",
        "        presented with its page number and chunk number. Returns an error message if\n",
        "        the query processing fails or if an error occurs during chunk retrieval.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If any error occurs during the process (e.g., issues with the embedding client,\n",
        "                   data format problems in the vector database).\n",
        "                   The specific error is printed to the console.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        \n",
        "        #!TBD IN WORKSHOP\n",
        "        pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting relevant chunks: {str(e)}\")\n",
        "        return \"Error retrieving relevant chunks\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hxyLlTjsstI"
      },
      "source": [
        "Let's test out our retrieval component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek4aF0Esck2H"
      },
      "source": [
        "- Let's try the same query for which the model was not able to answer earlier, due to lack of context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSd8ZeH6D7m4"
      },
      "outputs": [],
      "source": [
        "query = \"What is the price of a basic tune-up at Cymbal Bikes?\"\n",
        "relevant_context = get_relevant_chunks(\n",
        "    query, vector_db_mini_vertex, client, text_embedding_model, top_k=3\n",
        ")\n",
        "relevant_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEfJkwSqJ5KR"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp7doymTJ7Iu"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(multiplier=1, max=120), stop=stop_after_attempt(4))\n",
        "async def generate_answer(\n",
        "    query: str, context: str, llm_client: Any, modality: str = \"text\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate answer using LLM with retry logic for API quota management.\n",
        "\n",
        "    Args:\n",
        "        query: User query.\n",
        "        context: Relevant text providing context for the query.\n",
        "        llm_client: Client for accessing LLM API.\n",
        "        modality: Output modality (text or audio).\n",
        "\n",
        "    Returns:\n",
        "        Generated answer.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an unexpected error occurs during the LLM call (after retry attempts are exhausted).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # If context indicates earlier quota issues, return early\n",
        "        if context in [\n",
        "            \"Could not process query due to quota issues\",\n",
        "            \"Error retrieving relevant chunks\",\n",
        "        ]:\n",
        "            return \"Can't Process, Quota Issues\"\n",
        "\n",
        "        #!TBD IN WORKSHOP\n",
        "        pass\n",
        "\n",
        "    except Exception as e:\n",
        "        if \"RESOURCE_EXHAUSTED\" in str(e):\n",
        "            return \"Can't Process, Quota Issues\"\n",
        "        print(f\"Error generating answer: {str(e)}\")\n",
        "        return \"Error generating answer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-iesR2BEHnI"
      },
      "outputs": [],
      "source": [
        "query = \"What is the price of a basic tune-up at Cymbal Bikes?\"\n",
        "\n",
        "generated_answer = await generate_answer(\n",
        "    query, relevant_context, client, modality=\"text\"\n",
        ")\n",
        "display(Markdown(generated_answer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MNlAoAHR0Do"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoOeqxETR2G_"
      },
      "outputs": [],
      "source": [
        "async def rag(\n",
        "    question: str,\n",
        "    vector_db: pd.DataFrame,\n",
        "    embedding_client: Any,\n",
        "    embedding_model: str,\n",
        "    llm_client: Any,\n",
        "    top_k: int,\n",
        "    llm_model: str,\n",
        "    modality: str = \"text\",\n",
        ") -> str | None:\n",
        "    \"\"\"\n",
        "    RAG Pipeline.\n",
        "\n",
        "    Args:\n",
        "        question: User query.\n",
        "        vector_db: DataFrame containing document chunks and embeddings.\n",
        "        embedding_client: Client for accessing embedding API.\n",
        "        embedding_model: Name of the embedding model.\n",
        "        llm_client: Client for accessing LLM API.\n",
        "        top_k: The number of top relevant chunks to retrieve from the vector database.\n",
        "        llm_model: Name of the LLM model.\n",
        "        modality: Output modality (text or audio).\n",
        "\n",
        "    Returns:\n",
        "        For text modality, generated answer.\n",
        "        For audio modality, audio playback widget.\n",
        "\n",
        "    Raises:\n",
        "        Exception:  Catches and prints any exceptions during processing. Returns an error message.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        #!TBD IN WORKSHOP\n",
        "        pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question '{question}': {str(e)}\")\n",
        "        return {\"question\": question, \"generated_answer\": \"Error processing question\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx_GwXESk9aP"
      },
      "outputs": [],
      "source": [
        "question_set = [\n",
        "    {\n",
        "        \"question\": \"What is the price of a basic tune-up at Cymbal Bikes?\",\n",
        "        \"answer\": \"A basic tune-up costs $100.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How much does it cost to replace a tire at Cymbal Bikes?\",\n",
        "        \"answer\": \"Replacing a tire at Cymbal Bikes costs $50 per tire.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What does gear repair at Cymbal Bikes include?\",\n",
        "        \"answer\": \"Gear repair includes inspection and repair of the gears, including replacement of chainrings, cogs, and cables as needed.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the cost of replacing a tube at Cymbal Bikes?\",\n",
        "        \"answer\": \"Replacing a tube at Cymbal Bikes costs $20.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can I return clothing items to Cymbal Bikes?\",\n",
        "        \"answer\": \"Clothing can only be returned if it is unworn and in the original packaging.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the time frame for returning items to Cymbal Bikes?\",\n",
        "        \"answer\": \"Cymbal Bikes offers a 30-day return policy on all items.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can I return edible items like energy gels?\",\n",
        "        \"answer\": \"No, edible items are not returnable.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How can I return an item purchased online from Cymbal Bikes?\",\n",
        "        \"answer\": \"Items purchased online can be returned to any Cymbal Bikes store or mailed back.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What should I include when returning an item to Cymbal Bikes?\",\n",
        "        \"answer\": \"Please include the original receipt and a copy of your shipping confirmation when returning an item.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Does Cymbal Bikes offer refunds for shipping charges?\",\n",
        "        \"answer\": \"Cymbal Bikes does not offer refunds for shipping charges, except for defective items.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How do I process a return for a defective item at Cymbal Bikes?\",\n",
        "        \"answer\": \"To process a return for a defective item, please contact Cymbal Bikes first.\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmyN-h18EZdT"
      },
      "outputs": [],
      "source": [
        "question_set[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f3hsHqBEbwc"
      },
      "outputs": [],
      "source": [
        "response = await rag(\n",
        "    question=question_set[0][\"question\"],\n",
        "    vector_db=vector_db_mini_vertex,\n",
        "    embedding_client=client,  # For embedding generation\n",
        "    embedding_model=text_embedding_model,  # For embedding model\n",
        "    llm_client=client,  # For answer generation,\n",
        "    top_k=3,\n",
        "    llm_model=MODEL,\n",
        "    modality=\"text\",\n",
        ")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use a ChatUI for a better inface and a continuous chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "async def get_agent_response(message, history):\n",
        "    time.sleep(0.05)\n",
        "    yield await rag(\n",
        "            question=message,\n",
        "            vector_db=vector_db_mini_vertex,\n",
        "            embedding_client=client,  # For embedding generation\n",
        "            embedding_model=text_embedding_model,  # For embedding model\n",
        "            llm_client=client,  # For answer generation,\n",
        "            top_k=3,\n",
        "            llm_model=MODEL,\n",
        "            modality=\"text\",\n",
        "        )\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    get_agent_response,\n",
        "    type=\"messages\",\n",
        "    flagging_mode=\"manual\",\n",
        "    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
        "    save_history=True,\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "real_time_rag_retail_gemini_2_0.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
